 @article{Finlayson_Chung_Kohane_Beam_2018, title={Adversarial Attacks Against Medical Deep Learning Systems}, url={http://arxiv.org/abs/1804.05296}, abstractNote={The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we argue that the field of medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud, we extend adversarial attacks to three popular medical imaging tasks, and we provide concrete examples of how and why such attacks could be realistically carried out. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. We urge caution in deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.}, note={arXiv: 1804.05296}, journal={arXiv:1804.05296 [cs, stat]}, author={Finlayson, Samuel G. and Chung, Hyung Won and Kohane, Isaac S. and Beam, Andrew L.}, year={2018}, month={04}}
